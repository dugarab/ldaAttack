{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities,matutils\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findVariationalParams(M,datapath,alpha,K):\n",
    "    ''' \n",
    "    Function to determine the variational parameters\n",
    "    Input: \n",
    "    M = DxV integer ndarray where \n",
    "           M(d,v) = Count of word v in document d\n",
    "    alpha = K-vector of floats, shape = (K,) \n",
    "    beta = V-vector of floats, shape = (V,)\n",
    "    \n",
    "    Method: Use eq. 2,3,4 to determine outputs\n",
    "    Refer to Blei et al.(2003) for definition of psi\n",
    "    \n",
    "    Output:\n",
    "    eta: KxV float ndarray\n",
    "    gamma: DxK float ndarray\n",
    "    phi: DxVxK float ndarray \n",
    "         (returning a row_sparse D.V x K matrix phisp) \n",
    "    '''\n",
    "    D,V = M.shape\n",
    "    #K = alpha.shape[0]\n",
    "    eta = np.ones ((K,V))\n",
    "    gamma = np.ones((D,K))\n",
    "    phi = np.ones((D,V,K))\n",
    "    ################### CODE HERE #######################\n",
    "    path = \"lda-c/\"\n",
    "    param  = \"param\"\n",
    "    cmd1 = \"rm -r \"+param\n",
    "    cmd2 = \"mkdir \"+param\n",
    "    cmd3 = path+\"lda est \"+str(alpha)+\" \"+ str(K) +\" \"+ path + \\\n",
    "          \"settings.txt \" + datapath + \" random\" + \" ./\" \\\n",
    "          + param\n",
    "    os.system(cmd1)\n",
    "    os.system(cmd2)\n",
    "    os.system(cmd3)\n",
    "    print(\"Reading phi\")\n",
    "    phi = np.loadtxt(\"param/final.phi\").reshape(D,V,K)\n",
    "    print(\"Reading gamma\")\n",
    "    gamma = np.loadtxt(\"param/final.gamma\")\n",
    "    print(\"Reading beta\")\n",
    "    beta = np.loadtxt(\"param/final.beta\")\n",
    "    print (\"Building eta\")\n",
    "    for v in xrange(V):\n",
    "        s=np.zeros((K,))\n",
    "        for d in xrange(D):\n",
    "            s+=M[d,v]*phi.T[:,v,d]\n",
    "        eta[:,v] = beta[:,v] + s\n",
    "    phisp = sp.csr_matrix(phi.reshape(D*V,K))\n",
    "    #Use phisp preferably for operations\n",
    "    #Usage phi[d,v,k]=phisp[d*V+v,k]\n",
    "    #####################################################\n",
    "    return eta,gamma,phisp,beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessWords(inputPath,corpusfile,stopwords):\n",
    "    '''\n",
    "    Parses all files in the inputPath folder and\n",
    "    returns the word matrix M:DxV of type ndarray(int32).\n",
    "    Also stores the corpus in blei's LDA-C format as \n",
    "    corpusfile (corpusfile is a full path with filename).\n",
    "    Input-specific stopwords also taken as array of strings\n",
    "    '''\n",
    "    porter = PorterStemmer()\n",
    "    docs,docLen=[],0\n",
    "    print(\"Reading data from %s\"%inputPath)\n",
    "    for path in inputPath:\n",
    "        for filename in os.listdir(path):\n",
    "            with open(path+filename,'r') as inp:\n",
    "                print(\"Reading data from %s\"%filename)\n",
    "                f=inp.read()\n",
    "                words=word_tokenize(f)\n",
    "                words = [w.lower() for w in words]\n",
    "                noPunc = [w.translate(None,string.punctuation)\n",
    "                          for w in words]\n",
    "                noEmp = [w for w in noPunc if w.isalpha()]\n",
    "                noStop = [w for w in noEmp if not w\n",
    "                          in stop_words]\n",
    "                stemmed = [porter.stem(w) for w in noStop]\n",
    "                stemmed = [w for w in stemmed if not w\n",
    "                          in stop_words]\n",
    "            docLen+=len(stemmed)\n",
    "            docs.append(stemmed)\n",
    "            #docs.append(noStop)\n",
    "    D = len(docs)\n",
    "    print (\"Total Number of documents = %d\"%D)\n",
    "    print(\"Average words per document = %d\"%(docLen/D))\n",
    "    dcy = corpora.Dictionary(docs)\n",
    "    V = len(dcy)\n",
    "    print(\"Total vocabulary size = %d\"%V)\n",
    "    #dcy.save(os.path.join(TMP,'cong.dict'))\n",
    "    corpus = [dcy.doc2bow(text) for text in docs]\n",
    "    corpora.BleiCorpus.serialize(corpusfile,corpus)\n",
    "    M = matutils.corpus2dense(corpus, num_terms=V, num_docs=D,\n",
    "                              dtype=np.int32).T\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0=time.time()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['mr','would','say','lt', 'p', 'gt',\n",
    "               'amp', 'nbsp','bill','speaker','us',\n",
    "               'going','act','gentleman','gentlewoman',\n",
    "               'chairman','nay','yea','thank']\n",
    "pathnames = ['./convote_v1.1/data_stage_one/'+wor+'/'\n",
    "             for wor in ['development_set','training_set']]\n",
    "os.system(\"rm -r datafiles\")\n",
    "os.system(\"mkdir datafiles\")\n",
    "PTH = \"./datafiles/congCorp.lda-c\"\n",
    "alpha = 0.1\n",
    "K = 10\n",
    "M = preprocessWords(pathnames,PTH,stopwords)\n",
    "eta,gamma,phisp,beta=findVariationalParams(M,PTH,alpha,K)\n",
    "t1=time.time()\n",
    "print (\"Time taken = %f\"%(t1-t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
